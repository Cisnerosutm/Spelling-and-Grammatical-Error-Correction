{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:44.213610Z",
     "start_time": "2018-11-03T14:21:44.210438Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import array\n",
    "import os\n",
    "import gzip, ubjson\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:46.317545Z",
     "start_time": "2018-11-03T14:21:46.305456Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    STARTING_QUOTES = [\n",
    "        (re.compile(r'^\\\"'), r'``'),\n",
    "        (re.compile(r'(``)'), r' \\1 '),\n",
    "        (re.compile(r\"([ \\(\\[{<])(\\\"|\\'{2})\"), r'\\1 `` '),\n",
    "    ]\n",
    "    # punctuation\n",
    "    PUNCTUATION = [\n",
    "        (re.compile(r'([:,])([^\\d])'), r' \\1 \\2'),\n",
    "        (re.compile(r'([:,])$'), r' \\1 '),\n",
    "        (re.compile(r'\\.\\.\\.'), r' ... '),\n",
    "        (re.compile(r'[;@#$%&]'), r' \\g<0> '),\n",
    "        (re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$'), r'\\1 \\2\\3 '),  # Handles the final period.\n",
    "        (re.compile(r'[?!]'), r' \\g<0> '),\n",
    "\n",
    "        (re.compile(r\"([^'])' \"), r\"\\1 ' \"),\n",
    "    ]\n",
    "    # Pads parentheses\n",
    "    PARENS_BRACKETS = (re.compile(r'[\\]\\[\\(\\)\\{\\}\\<\\>]'), r' \\g<0> ')\n",
    "    # Optionally: Convert parentheses, brackets and converts them to PTB symbols.\n",
    "    CONVERT_PARENTHESES = [\n",
    "        (re.compile(r'\\('), '-LRB-'), (re.compile(r'\\)'), '-RRB-'),\n",
    "        (re.compile(r'\\['), '-LSB-'), (re.compile(r'\\]'), '-RSB-'),\n",
    "        (re.compile(r'\\{'), '-LCB-'), (re.compile(r'\\}'), '-RCB-')\n",
    "    ]\n",
    "    DOUBLE_DASHES = (re.compile(r'--'), r' -- ')\n",
    "    # ending quotes\n",
    "    ENDING_QUOTES = [\n",
    "        (re.compile(r'\"'), \" '' \"),\n",
    "        (re.compile(r'(\\S)(\\'\\')'), r'\\1 \\2 '),\n",
    "        (re.compile(r\"([^' ])('[sS]|'[mM]|'[dD]|') \"), r\"\\1 \\2 \"),\n",
    "        (re.compile(r\"([^' ])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T) \"), r\"\\1 \\2 \"),\n",
    "    ]\n",
    "    # List of contractions adapted from Robert MacIntyre's tokenizer.\n",
    "    CONTRACTIONS2 = list(map(re.compile, [r\"(?i)\\b(can)(?#X)(not)\\b\",\n",
    "                         r\"(?i)\\b(d)(?#X)('ye)\\b\",\n",
    "                         r\"(?i)\\b(gim)(?#X)(me)\\b\",\n",
    "                         r\"(?i)\\b(gon)(?#X)(na)\\b\",\n",
    "                         r\"(?i)\\b(got)(?#X)(ta)\\b\",\n",
    "                         r\"(?i)\\b(lem)(?#X)(me)\\b\",\n",
    "                         r\"(?i)\\b(mor)(?#X)('n)\\b\",\n",
    "                         r\"(?i)\\b(wan)(?#X)(na)\\s\"]))\n",
    "    CONTRACTIONS3 = list(map(re.compile, [r\"(?i) ('t)(?#X)(is)\\b\", r\"(?i) ('t)(?#X)(was)\\b\"]))\n",
    "    for regexp, substitution in STARTING_QUOTES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    for regexp, substitution in PUNCTUATION:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    # Handles parentheses.\n",
    "    regexp, substitution = PARENS_BRACKETS\n",
    "    text = regexp.sub(substitution, text)\n",
    "    # Optionally convert parentheses\n",
    "    for regexp, substitution in CONVERT_PARENTHESES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    # Handles double dash.\n",
    "    regexp, substitution = DOUBLE_DASHES\n",
    "    text = regexp.sub(substitution, text)\n",
    "    # add extra space to make things easier\n",
    "    text = \" \" + text + \" \"\n",
    "    for regexp, substitution in ENDING_QUOTES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    for regexp in CONTRACTIONS2:\n",
    "        text = regexp.sub(r' \\1 \\2 ', text)\n",
    "    for regexp in CONTRACTIONS3:\n",
    "        text = regexp.sub(r' \\1 \\2 ', text)\n",
    "    tokens=text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def find_sentences(text_file):\n",
    "    output_file=\"segmented_sents.txt\"\n",
    "    cmd = \"segmenter \"+text_file+\" > \"+output_file\n",
    "    os.system(cmd)\n",
    "    sentences=[]\n",
    "    with open(output_file,\"r\") as fl:\n",
    "        for line in fl:\n",
    "            if len(line.strip())>0:\n",
    "                sentences.append(line.strip())\n",
    "    fl.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:48.021218Z",
     "start_time": "2018-11-03T14:21:48.014824Z"
    }
   },
   "outputs": [],
   "source": [
    "def rename_files(data_path):\n",
    "    i=1\n",
    "    for file in os.listdir(data_path):\n",
    "        os.rename(os.path.join(data_path, file), os.path.join(data_path, str(i)+'.txt'))\n",
    "        i+=1\n",
    "    \n",
    "def extract_data(data_path):\n",
    "    data=data_path\n",
    "    corpus=[]\n",
    "    cnt=2\n",
    "    for file in os.listdir(data):\n",
    "        for sent in find_sentences(data+file):\n",
    "            #print(sent)\n",
    "            corpus.append(tokenize(sent))\n",
    "        #cnt+=-1\n",
    "        #if cnt==0:\n",
    "            #break        \n",
    "    return corpus \n",
    "\n",
    "def load(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return ubjson.load(f)\n",
    "    \n",
    "def save(filename,obj):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        ubjson.dump(obj, f) \n",
    "        \n",
    "def find_unique_tokens(corpus):\n",
    "    unique_tokens=[]\n",
    "    for sent in corpus:\n",
    "        for token in sent:\n",
    "            if token not in unique_tokens:\n",
    "                unique_tokens.append(token)\n",
    "    return unique_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:37:37.318565Z",
     "start_time": "2018-11-03T15:37:37.314377Z"
    }
   },
   "outputs": [],
   "source": [
    "script_path=os.path.dirname(os.path.abspath('__file__'))\n",
    "data=script_path+\"/Gutenberg/txt/\"\n",
    "#rename_files(data)\n",
    "#corpus = extract_data(data)\n",
    "#save(\"corpus\",corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-03T15:37:37.719Z"
    }
   },
   "outputs": [],
   "source": [
    "#corpus = load(\"corpus\")\n",
    "unique_tokens = find_unique_tokens(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE N-gram counts and probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.651852Z",
     "start_time": "2018-11-03T14:20:16.252Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_ngrams(sent_tags, n):\n",
    "    ngrams = list(zip(*[sent_tags[i:] for i in range(n)]))\n",
    "    return ngrams\n",
    "\n",
    "def calc_MLE_counts(corpus,lvl=\"word\"):\n",
    "    unigram_c = defaultdict(int)\n",
    "    bigram_c = defaultdict(int)\n",
    "    trigram_c = defaultdict(int)\n",
    "    if lvl!=\"word\":\n",
    "        for sent in corpus:\n",
    "            for word in sent:\n",
    "                \n",
    "                unigrams=calc_ngrams(list(word),1)\n",
    "                bigrams=calc_ngrams(list(word),2)\n",
    "                trigrams=calc_ngrams(list(word),3)\n",
    "                \n",
    "                for unigram in unigrams:\n",
    "                    unigram_c[unigram] += 1\n",
    "\n",
    "                for bigram in bigrams:\n",
    "                    bigram_c[bigram] += 1\n",
    "\n",
    "                for trigram in trigrams:\n",
    "                    trigram_c[trigram] += 1\n",
    "\n",
    "    else:    \n",
    "        for sent in corpus:\n",
    "            unigrams = calc_ngrams(sent, 1)\n",
    "            bigrams = calc_ngrams(sent, 2)\n",
    "            trigrams = calc_ngrams(sent, 3)\n",
    "\n",
    "            for unigram in unigrams:\n",
    "                unigram_c[unigram] += 1\n",
    "\n",
    "            for bigram in bigrams:\n",
    "                bigram_c[bigram] += 1\n",
    "\n",
    "            for trigram in trigrams:\n",
    "                trigram_c[trigram] += 1\n",
    "\n",
    "    return unigram_c, bigram_c, trigram_c\n",
    "\n",
    "def ml_prob(ngram,counts): ## ngram :- [w1,w2,w3]\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        if ngram in unigrams:\n",
    "            prob=unigrams[ngram]/float(sum(unigrams.values()))\n",
    "        else:\n",
    "            prob=0\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        w1=ngram[0]\n",
    "        w2=ngram[1]\n",
    "        if ngram in bigrams:\n",
    "            prob = bigrams[ngram]/float(unigrams[ngram[0:1]])\n",
    "        else:\n",
    "            prob=0\n",
    "    else:\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        if ngram in trigrams:\n",
    "            prob=trigrams[ngram]/float(bigrams[ngram[0:2]])\n",
    "        else:\n",
    "            prob=0\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.652755Z",
     "start_time": "2018-11-03T14:20:16.254Z"
    }
   },
   "outputs": [],
   "source": [
    "word_uni,word_bi,word_tri = calc_MLE_counts(corpus)\n",
    "ch_uni,ch_bi,ch_tri = calc_MLE_counts(corpus,\"char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.653721Z",
     "start_time": "2018-11-03T14:20:16.259Z"
    }
   },
   "outputs": [],
   "source": [
    "def laplace_smoothing(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        N=sum(unigrams.values())\n",
    "        V=len(unigrams)\n",
    "        if ngram in unigrams:\n",
    "            prob = (unigrams[ngram]+1)/float(N+V)\n",
    "        else:\n",
    "            prob = 1/float(N+V)\n",
    "\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        V=len(unigrams)\n",
    "        if ngram in bigrams:\n",
    "            prob = (bigrams[ngram]+1)/float(unigrams[ngram[0:1]]+V)\n",
    "        else:\n",
    "            if ngram[0:1] in unigrams:\n",
    "                prob = 1/float(unigrams[ngram[0:1]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        V=len(unigrams)\n",
    "        if ngram in trigrams:\n",
    "            prob = (trigrams[ngram]+1)/float(bigrams[ngram[0:2]]+V)\n",
    "        else:\n",
    "            if ngram[0:2] in bigrams:\n",
    "                prob = 1/float(bigrams[ngram[0:2]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    return prob\n",
    "\n",
    "def good_turing(ngram,counts):\n",
    "    V=sum(counts[0].values())\n",
    "    ngram=tuple(ngram)\n",
    "    l=len(ngram)\n",
    "    ngrams=counts[l-1]\n",
    "    N=sum(ngrams.values())\n",
    "    nrs={}\n",
    "    for k,v in ngrams.items():\n",
    "        if v not in nrs:\n",
    "            nrs[v].append(k) ## dictionary of (key=freq of ngram) and (value = list of all ngram tuple whose freq. is key)\n",
    "    nr_counts={k:len(v) for k,v in nrs.items()}\n",
    "    if 0 not in nr_counts:\n",
    "        nr_counts[0]=V**l - N\n",
    "    else:\n",
    "        nr_counts[0]+=V**l - N\n",
    "    MAX=sorted(nr_counts.items())[0][1] ##max freq of ngram in corpus\n",
    "    new_nrs={}\n",
    "    for r, nr in nr_counts.items():\n",
    "        if (r+1) in nr_counts:\n",
    "            new_nr=(r+1)*nr_counts[r+1]/float(N)\n",
    "        else:\n",
    "            new_nr=MAX*r**-2/float(N)\n",
    "        new_nrs[r]=new_nr\n",
    "\n",
    "    if ngrams[ngram]>5:\n",
    "        prob=ml_prob(ngram,counts)\n",
    "    else:\n",
    "        denominator=(1 - 6*new_nrs[6]/float(new_nrs[1]))/float(N)\n",
    "        if ngram in ngrams:\n",
    "            numerator=(ngrams[ngram]+1)*new_nrs[ngrams[ngram]+1]/float(new_nrs[ngrams[ngram]])\n",
    "            mod_num=num - ngrams[ngram]*6*new_nrs[6]/float(new_nrs[1])\n",
    "            prob = mod_num/float(denominator)\n",
    "        else:\n",
    "            prob = new_nrs[1]/(new_nrs[0]*float(denominator)) ##not considering singleton ngrams as unseen\n",
    "    return prob\n",
    "\n",
    "def witten_bell(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        ngram_count=bigrams[ngram]\n",
    "        prior_count=unigrams[ngram[0:1]]\n",
    "        type_count=0\n",
    "        s=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[0:1]:\n",
    "                type_count+=1\n",
    "                s+=bigrams[bigram]\n",
    "        vocab_size=len(bigrams)\n",
    "        z = vocab_size - type_count\n",
    "        if ngram_count==0:\n",
    "            prob = type_count/float(z*(prior_count+type_count))\n",
    "        else:\n",
    "            prob = ngram_count/float(prior_count + type_count)\n",
    "        wb_lambda=1-bigrams[bigram]/float(bigrams[bigram]+s)\n",
    "        prob=(wb_lambda)*prob+(1-wb_lambda)*unigrams[ngram[1:2]]/float(sum(unigrams.values()))\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        ngram_count = trigrams[ngram]\n",
    "        prior_count = bigrams[ngram[0:2]]\n",
    "        type_count=0\n",
    "        s=0\n",
    "        for trigram in trigrams:\n",
    "            if ngram[0:1]==trigram[0] and ngram[1:2]==trigram[1]:\n",
    "                type_count+=1\n",
    "                s+=trigrams[trigram]\n",
    "        vocab_size = len(trigrams)\n",
    "        z = vocab_size - type_count\n",
    "        if ngram_count == 0:\n",
    "            prob = type_count/float(z*(prior_count + type_count))\n",
    "        else:\n",
    "            prob = ngram_count/float(prior_count + type_count)\n",
    "        wb_lambda=1-trigrams[trigram]/float(trigrams[trigram]+s)\n",
    "        prob=(wb_lambda)*prob+(1-wb_lambda)*bigrams[ngram[1:3]]/float(unigrams[ngram[1:2]])\n",
    "    return prob\n",
    "\n",
    "def stupid_backoff(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    ans=0\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        if ngram in bigrams:\n",
    "            denominator=0\n",
    "            for bigram in bigrams:\n",
    "                if ngram[0:1]==bigram[0]:\n",
    "                    denominator+=bigrams[bigram]\n",
    "            ans=bigram[ngram]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[1:2] in unigrams:\n",
    "                ans = 0.4 * unigrams[ngram[1:2]]/float(sum(unigrams.values()))\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        if ngram in trigrams:\n",
    "            denominator=0\n",
    "            for trigram in trigrams:\n",
    "                if ngram[0:1]==trigram[0] and ngram[1:2]==trigram[1]:\n",
    "                    denominator+=trigrams[trigram]\n",
    "            ans=trigrams[ngram]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[1:3] in bigrams:\n",
    "                denominator=0\n",
    "                for bigram in bigrams:\n",
    "                    if ngram[1:2]==bigram[0]:\n",
    "                        denominator+=bigrams[bigram]\n",
    "                ans=0.4*bigrams[ngram[1:3]]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[2:3] in unigrams:\n",
    "                ans = 0.16*unigrams[ngram[2:3]]/float(sum(unigrams.values()))\n",
    "    return ans\n",
    "\n",
    "def katz_backoff(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        r = bigrams[ngram]\n",
    "        k=5\n",
    "        ngrams=bigrams\n",
    "        N=sum(ngrams.values())\n",
    "        nrs={}\n",
    "        for k,v in ngrams.items():\n",
    "            if v not in nrs:\n",
    "                nrs[v].append(k) ## dictionary of key as freq of ngram and value as list of all ngram tuple\n",
    "        nr_counts={k:len(v) for k,v in nrs.items()}\n",
    "        MAX=sorted(nr_counts.items())[0][1] ##max freq of ngram in corpus\n",
    "        new_nrs={}\n",
    "        for r, nr in nr_counts.items():\n",
    "            if (r+1) in nr_counts:\n",
    "                new_nr=(r+1)*nr_counts[r+1]/float(N)\n",
    "            else:\n",
    "                new_nr=MAX*r**-2/float(N)\n",
    "            new_nrs[r]=new_nr\n",
    "        num1 = (r+1)*new_nrs[r+1]/float(r*new_nrs[r])\n",
    "        num2 = (k+1)*new_nrs[k+1]/float(new_nrs[1])\n",
    "        deno =  1 - (k+1)*new_nrs[k+1]/float(new_nrs[1])\n",
    "        dr = (num1 - num2)/float(deno)\n",
    "        s=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[0:1]:\n",
    "                s+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        numerator=1-s\n",
    "        s=0\n",
    "        for unigram in unigrams:\n",
    "            s+=good_turing([unigram[0]],counts)\n",
    "        denominator=1-s\n",
    "        alpha = numerator/float(denominator)\n",
    "        if r>k:\n",
    "            prob = good_turing([ngram[0:1][0],ngram[1:2][0]],counts)\n",
    "        elif r>0 and r<=k:\n",
    "            prob = dr*good_turing([ngram[0:1][0],ngram[1:2][0]],counts)\n",
    "        else:\n",
    "            prob = alpha*good_turing([ngram[1:2][0]],counts)\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        r1 = trigrams[ngram]\n",
    "        r2= bigrams[ngram[1:3]]\n",
    "        s1=0\n",
    "        for trigram in trigrams:\n",
    "            if trigram[0]==ngram[0:1] and trigram[1]==ngram[1:2]:\n",
    "                s1+=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "        num=1-s1\n",
    "        s1=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[1:2]:\n",
    "                s1+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        deno=1-s1\n",
    "        alpha1=num/float(deno)\n",
    "        s2=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[1:2]:\n",
    "                s+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        numerator=1-s\n",
    "        s2=0\n",
    "        for unigram in unigrams:\n",
    "            s2+=good_turing([unigram[0]],counts)\n",
    "        denominator=1-s\n",
    "        alpha2 = numerator/float(denominator)\n",
    "        if r1>0:\n",
    "            prob=good_turing([ngram[0:1][0],ngram[1:2][0],ngram[2:3][0]],counts)\n",
    "        elif r1==0 and r2>0:\n",
    "            prob=alpha1*good_turing([ngram[1:2][0],ngram[2:3][0]],counts)\n",
    "        else:\n",
    "            prob=alpha2*good_turing([ngram[2:3][0]],counts)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.654672Z",
     "start_time": "2018-11-03T14:20:16.264Z"
    }
   },
   "outputs": [],
   "source": [
    "def language_model(model_name,counts,n):\n",
    "    ##model_name :- \"lap\",\"gt\",\"wb\", \"sbo\", \"kbo\" [laplace/add-1,good-turing,witten_bell,stupid_backoff,katz-backoff]\n",
    "    unigrams=counts[0]\n",
    "    bigrams=counts[1]\n",
    "    trigrams=counts[2]\n",
    "    unigram_prob=defaultdict(float)\n",
    "    bigram_prob=defaultdict(float)\n",
    "    trigram_prob=defaultdict(float)\n",
    "    if n==1:\n",
    "        for unigram in unigrams:\n",
    "            if model_name==\"lap\":\n",
    "                unigram_prob[unigram]=laplace_smoothing([unigram[0]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                unigram_prob[unigram]=good_turing([unigram[0]],counts)\n",
    "        return unigram_prob\n",
    "\n",
    "    elif n==2:\n",
    "        for bigram in bigrams:\n",
    "            if model_name==\"lap\":\n",
    "                bigram_prob[bigram]=laplace_smoothing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                bigram_prob[bigram]=good_turing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                bigram_prob[bigram]=witten_bell([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                bigram_prob[bigram]=stupid_backoff([bigram[0],bigram[1]],counts)\n",
    "            else:\n",
    "                bigram_prob[bigram]=katz_backoff([bigram[0],bigram[1]],counts)\n",
    "        return bigram_prob\n",
    "\n",
    "    else:\n",
    "        for trigram in trigrams:\n",
    "            if model_name==\"lap\":\n",
    "                trigram_prob[trigram]=laplace_smoothing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                trigram_prob[trigram]=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                trigram_prob[trigram]=witten_bell([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                trigram_prob[trigram]=stupid_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            else:\n",
    "                trigram_prob[trigram]=katz_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "        return trigram_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.655647Z",
     "start_time": "2018-11-03T14:20:16.266Z"
    }
   },
   "outputs": [],
   "source": [
    "word_uni_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],1)\n",
    "word_uni_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],1)\n",
    "\n",
    "word_bi_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_wb = language_model(\"wb\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_sbo = language_model(\"sbo\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_kbo = language_model(\"kbo\",[word_uni,word_bi,word_tri],2)\n",
    "\n",
    "word_tri_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_wb = language_model(\"wb\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_sbo = language_model(\"sbo\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_kbo = language_model(\"kbo\",[word_uni,word_bi,word_tri],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find next word after given N-1 grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.656499Z",
     "start_time": "2018-11-03T14:20:16.270Z"
    }
   },
   "outputs": [],
   "source": [
    "def complete_ngram(ngram,counts,smoothing_method): ##ngram = [w1,w2] or [w1]\n",
    "# Given (N-1) gram, and the value 'N', print the possibilities that complete the n-gram\n",
    "# and plot them in decresing order of frequency\n",
    "    next_gram=[]\n",
    "    unigrams=counts[0]\n",
    "    for word in unigrams:\n",
    "        if len(ngram)==2:\n",
    "            tokens=[ngram[0],ngram[1],word[0]]\n",
    "        else:\n",
    "            tokens=[ngram[0],word[0]]\n",
    "        if smoothing_method==\"lap\":\n",
    "            prob = laplace_smoothing(tokens,counts)\n",
    "        elif smoothing_method==\"gt\":\n",
    "            prob = good_turing(tokens,counts)\n",
    "        elif smoothing_method==\"wb\":\n",
    "            prob = witten_bell(tokens,counts)\n",
    "        elif smoothing_method==\"sbo\":\n",
    "            prob=stupid_backoff(tokens,counts)\n",
    "        else:\n",
    "            prob=katz_backoff(token,counts)\n",
    "        next_gram.append((prob,word[0]))\n",
    "    next_gram.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    return next_gram[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.657412Z",
     "start_time": "2018-11-03T14:20:16.271Z"
    }
   },
   "outputs": [],
   "source": [
    "inp=\"The man\"\n",
    "ngram=inp.split()\n",
    "counts=[word_uni,word_bi,word_tri]\n",
    "smoothing_method=[\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\"]\n",
    "for smoothing in smoothing_method:\n",
    "    print(\"Smoothing: \",smoothing)\n",
    "    print(complete_ngram(ngram,counts,smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T23:04:40.546247Z",
     "start_time": "2018-11-02T23:04:40.531326Z"
    }
   },
   "source": [
    "# Edit Distance and candidates words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.658291Z",
     "start_time": "2018-11-03T14:20:16.275Z"
    }
   },
   "outputs": [],
   "source": [
    "def edit_distance(word,letters):\n",
    "    #letters    = 'abcdefghijklmnopqrstuvwxyz' ##add additional characters\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def find_candidates(word,unique_tokens):\n",
    "    ##Fusion words can also be computed in order to enhance the list\n",
    "    letters=[]\n",
    "    for token in unique_tokens:\n",
    "        for ch in list(token):\n",
    "            if ch not in letters:\n",
    "                letters.append(ch)\n",
    "    letters=\"\".join(str(ch) for ch in letters)\n",
    "    c4=set(word)\n",
    "    if word in unique_tokens:\n",
    "        c1=set(word)\n",
    "    edit1=edit_distance(word,letters)\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "    for w1 in edit1:\n",
    "        for w2 in edit_distance(w1,letters):\n",
    "            if w2 in unique_tokens:\n",
    "                tmp2.append(w2)\n",
    "        if w1 in unique_tokens:\n",
    "            tmp1.append(w1)\n",
    "    c2=set(tmp1)\n",
    "    c3=set(temp2)\n",
    "    return list(c1|c2|c3|c4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.659242Z",
     "start_time": "2018-11-03T14:20:16.279Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def spell_checker(word,n,model_name,counts,unique_tokens):\n",
    "# Given a word check if the spelling is correct using your Language model\n",
    "# Feel free to add more helper functions for this task\n",
    "# Giving candidate word with maximum probability using character L.M\n",
    "# word level L.M is not computed and comapred against character level which is left for future task\n",
    "    candidates=find_candidates(word,unique_tokens)\n",
    "    cand_probs=[]\n",
    "    for cand in candidates:\n",
    "        cand_ch_grams=[cand[i:i+n] for i in range(len(cand)-n+1)]\n",
    "        if n==1:\n",
    "            prod=1\n",
    "            for unigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([unigram],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([unigram],counts)\n",
    "        elif n==2:\n",
    "            prod=1\n",
    "            first_ch = list(cand_ch_grams[0])[0]\n",
    "            prod*=good_turing([first_ch],counts)\n",
    "            for bigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"wb\":\n",
    "                    prod*=witten_bell([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"sbo\":\n",
    "                    prod*=stupid_backoff([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                else:\n",
    "                    prod*=katz_backoff([list(bigram)[0],list(bigram)[1]],counts)\n",
    "\n",
    "        else:\n",
    "            prod=1\n",
    "            first_ch=list(cand_ch_grams[0])[0]\n",
    "            second_ch=list(cand_ch_grams[0])[1]\n",
    "            prod*=good_turing([first_ch],counts)\n",
    "            prod*=good_turing([first_ch,second_ch],counts)\n",
    "            for trigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"wb\":\n",
    "                    prod*=witten_bell([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"sbo\":\n",
    "                    prod*=stupid_backoff([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                else:\n",
    "                    prod*=katz_backoff([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "\n",
    "        cand_probs.append((prob,cand))\n",
    "    cand_probs.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    return cand_probs[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.660108Z",
     "start_time": "2018-11-03T14:20:16.281Z"
    }
   },
   "outputs": [],
   "source": [
    "input_word=\"catsq\"\n",
    "model_comb=[(\"lap\",\"gt\"),(\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\")]\n",
    "ns=[1,2,3]\n",
    "counts=[ch_uni,ch_bi,ch_tri]\n",
    "for n in ns:\n",
    "    print(\"N: \\n\",n)\n",
    "    if n==1:\n",
    "        model_names = model_comb[0]\n",
    "    else:\n",
    "        model_names = model_comb[1]\n",
    "    for model in model_names:\n",
    "        print(\"Smoothing: \",model)\n",
    "        print(spell_checker(input_word,n,model,counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammaticality of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.661045Z",
     "start_time": "2018-11-03T14:20:16.285Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_grammaticality(sentence,n,model_name,counts):\n",
    "  # Given a sentence, Build a model from the data which can give a score of grammaticality.\n",
    "  # More grammatical the sentence, better the score\n",
    "  # Feel free to add helper functions\n",
    "    n_grams = list(zip(*[sentence.split()[i:] for i in range(n)]))\n",
    "    if n==1:\n",
    "        prod=1\n",
    "        for unigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([unigram[0]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([unigram[0]],counts)\n",
    "    elif n==2:\n",
    "        prod=1\n",
    "        first_word = n_grams[0][0]\n",
    "        prod*=good_turing([first_word],counts)\n",
    "        for bigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                prod*=witten_bell([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                prod*=stupid_backoff([bigram[0],bigram[1]],counts)\n",
    "            else:\n",
    "                prod*=katz_backoff([bigram[0],bigram[1]],counts)\n",
    "\n",
    "    else:\n",
    "        prod=1\n",
    "        first_word= n_grams[0][0]\n",
    "        second_word=ngrams[0][1]\n",
    "        prod*=good_turing([first_word],counts)\n",
    "        prod*=good_turing([first_word,second_word],counts)\n",
    "        for trigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                prod*=witten_bell([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                prod*=stupid_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            else:\n",
    "                prod*=katz_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.661915Z",
     "start_time": "2018-11-03T14:20:16.287Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1='I have a red apple'\n",
    "sentence2='apple a have I red'\n",
    "print(\"Sentence1 :\",sentence1)\n",
    "print(\"Sentence2 :\",sentence2)\n",
    "\n",
    "model_comb=[(\"lap\",\"gt\"),(\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\")]\n",
    "ns=[1,2,3]\n",
    "counts=[word_uni,word_bi,word_tri]\n",
    "for n in ns:\n",
    "    print(\"N: \\n\",n)\n",
    "    if n==1:\n",
    "        model_names = model_comb[0]\n",
    "    else:\n",
    "        model_names = model_comb[1]\n",
    "    for model in model_names:\n",
    "        print(\"Smoothing: \",model)\n",
    "        print(\"Sentence1 Score: \",score_grammaticality(sentence1,n,model,counts))\n",
    "        print(\"Sentence2 Score: \",score_grammaticality(sentence2,n,model,counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
