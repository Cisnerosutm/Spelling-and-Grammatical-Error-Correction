{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:32:13.701828Z",
     "start_time": "2018-12-27T18:32:13.150354Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import array\n",
    "import os\n",
    "import gzip, json\n",
    "import numpy as np\n",
    "import pickle,itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:32:14.761028Z",
     "start_time": "2018-12-27T18:32:14.726789Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    STARTING_QUOTES = [\n",
    "        (re.compile(r'^\\\"'), r'``'),\n",
    "        (re.compile(r'(``)'), r' \\1 '),\n",
    "        (re.compile(r\"([ \\(\\[{<])(\\\"|\\'{2})\"), r'\\1 `` '),\n",
    "    ]\n",
    "    # punctuation\n",
    "    PUNCTUATION = [\n",
    "        (re.compile(r'([:,])([^\\d])'), r' \\1 \\2'),\n",
    "        (re.compile(r'([:,])$'), r' \\1 '),\n",
    "        (re.compile(r'\\.\\.\\.'), r' ... '),\n",
    "        (re.compile(r'[;@#$%&]'), r' \\g<0> '),\n",
    "        (re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$'), r'\\1 \\2\\3 '),  # Handles the final period.\n",
    "        (re.compile(r'[?!]'), r' \\g<0> '),\n",
    "\n",
    "        (re.compile(r\"([^'])' \"), r\"\\1 ' \"),\n",
    "    ]\n",
    "    # Pads parentheses\n",
    "    PARENS_BRACKETS = (re.compile(r'[\\]\\[\\(\\)\\{\\}\\<\\>]'), r' \\g<0> ')\n",
    "    # Optionally: Convert parentheses, brackets and converts them to PTB symbols.\n",
    "    CONVERT_PARENTHESES = [\n",
    "        (re.compile(r'\\('), '-LRB-'), (re.compile(r'\\)'), '-RRB-'),\n",
    "        (re.compile(r'\\['), '-LSB-'), (re.compile(r'\\]'), '-RSB-'),\n",
    "        (re.compile(r'\\{'), '-LCB-'), (re.compile(r'\\}'), '-RCB-')\n",
    "    ]\n",
    "    DOUBLE_DASHES = (re.compile(r'--'), r' -- ')\n",
    "    # ending quotes\n",
    "    ENDING_QUOTES = [\n",
    "        (re.compile(r'\"'), \" '' \"),\n",
    "        (re.compile(r'(\\S)(\\'\\')'), r'\\1 \\2 '),\n",
    "        (re.compile(r\"([^' ])('[sS]|'[mM]|'[dD]|') \"), r\"\\1 \\2 \"),\n",
    "        (re.compile(r\"([^' ])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T) \"), r\"\\1 \\2 \"),\n",
    "    ]\n",
    "    # List of contractions adapted from Robert MacIntyre's tokenizer.\n",
    "    CONTRACTIONS2 = list(map(re.compile, [r\"(?i)\\b(can)(?#X)(not)\\b\",\n",
    "                         r\"(?i)\\b(d)(?#X)('ye)\\b\",\n",
    "                         r\"(?i)\\b(gim)(?#X)(me)\\b\",\n",
    "                         r\"(?i)\\b(gon)(?#X)(na)\\b\",\n",
    "                         r\"(?i)\\b(got)(?#X)(ta)\\b\",\n",
    "                         r\"(?i)\\b(lem)(?#X)(me)\\b\",\n",
    "                         r\"(?i)\\b(mor)(?#X)('n)\\b\",\n",
    "                         r\"(?i)\\b(wan)(?#X)(na)\\s\"]))\n",
    "    CONTRACTIONS3 = list(map(re.compile, [r\"(?i) ('t)(?#X)(is)\\b\", r\"(?i) ('t)(?#X)(was)\\b\"]))\n",
    "    for regexp, substitution in STARTING_QUOTES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    for regexp, substitution in PUNCTUATION:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    # Handles parentheses.\n",
    "    regexp, substitution = PARENS_BRACKETS\n",
    "    text = regexp.sub(substitution, text)\n",
    "    # Optionally convert parentheses\n",
    "    for regexp, substitution in CONVERT_PARENTHESES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    # Handles double dash.\n",
    "    regexp, substitution = DOUBLE_DASHES\n",
    "    text = regexp.sub(substitution, text)\n",
    "    # add extra space to make things easier\n",
    "    text = \" \" + text + \" \"\n",
    "    for regexp, substitution in ENDING_QUOTES:\n",
    "        text = regexp.sub(substitution, text)\n",
    "    for regexp in CONTRACTIONS2:\n",
    "        text = regexp.sub(r' \\1 \\2 ', text)\n",
    "    for regexp in CONTRACTIONS3:\n",
    "        text = regexp.sub(r' \\1 \\2 ', text)\n",
    "    tokens=text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def find_sentences(text_file):\n",
    "    output_file=\"segmented_sents.txt\"\n",
    "    cmd = \"segmenter \"+text_file+\" > \"+output_file\n",
    "    os.system(cmd)\n",
    "    sentences=[]\n",
    "    with open(output_file,\"r\") as fl:\n",
    "        for line in fl:\n",
    "            if len(line.strip())>0:\n",
    "                sentences.append(line.strip())\n",
    "    fl.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:32:15.852103Z",
     "start_time": "2018-12-27T18:32:15.846415Z"
    }
   },
   "outputs": [],
   "source": [
    "def rename_files(data_path):\n",
    "    i=1\n",
    "    for file in os.listdir(data_path):\n",
    "        os.rename(os.path.join(data_path, file), os.path.join(data_path, str(i)+'.txt'))\n",
    "        i+=1\n",
    "    \n",
    "def extract_data(data_path):\n",
    "    data=data_path\n",
    "    corpus=[]\n",
    "    cnt=2\n",
    "    for file in os.listdir(data):\n",
    "        for sent in find_sentences(data+file):\n",
    "            #print(sent)\n",
    "            corpus.append(tokenize(sent))\n",
    "        #cnt+=-1\n",
    "        #if cnt==0:\n",
    "            #break        \n",
    "    return corpus \n",
    "\n",
    "def load(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return ubjson.load(f)\n",
    "    \n",
    "def save(filename,obj):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        ubjson.dump(obj, f) \n",
    "\n",
    "def calc_ngrams(sent_tags, n):\n",
    "    ngrams = list(zip(*[sent_tags[i:] for i in range(n)]))\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:32:16.424012Z",
     "start_time": "2018-12-27T18:32:16.421704Z"
    }
   },
   "outputs": [],
   "source": [
    "script_path=os.path.dirname(os.path.abspath('__file__'))\n",
    "data=script_path+\"/Gutenberg/txt/\"\n",
    "#rename_files(data)\n",
    "#corpus = extract_data(data)\n",
    "#save(\"corpus.txt\",corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:33:08.983401Z",
     "start_time": "2018-12-27T18:32:17.525492Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = load(\"corpus.txt\")\n",
    "unique_tokens = set(x for l in corpus for x in l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATISTICS OF DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:34:05.841616Z",
     "start_time": "2018-12-27T18:33:08.986008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences:  12760181\n",
      "Total unigrams/words:  124163963\n",
      "Total bigrams:  222807564\n",
      "Total trigrams:  296685381\n",
      "Unique tokens:  654194\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences: \",len(corpus))\n",
    "print(\"Total unigrams/words: \",sum([len(sent) for sent in corpus]))\n",
    "print(\"Total bigrams: \",sum([len(ngrams) for sent in corpus for ngrams in calc_ngrams(sent,2)]))\n",
    "print(\"Total trigrams: \",sum([len(ngrams) for sent in corpus for ngrams in calc_ngrams(sent,3)]))\n",
    "print(\"Unique tokens: \",len(unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate zipf and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:43:01.176176Z",
     "start_time": "2018-12-27T18:42:45.259199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequency = Counter(itertools.chain(*corpus))\n",
    "s = frequency.values()\n",
    "s = np.array(list(s))\n",
    "a = 2. #  distribution parameter\n",
    "count, bins, ignored = plt.hist(s[s<50], 50, density=True)\n",
    "x = np.arange(1., 50.)\n",
    "y = x**(-a) / special.zetac(a)\n",
    "plt.plot(x, y/max(y), linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE N-gram counts and probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T18:43:23.328687Z",
     "start_time": "2018-12-27T18:43:23.318341Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_MLE_counts(corpus,lvl=\"word\"):\n",
    "    unigram_c = defaultdict(int)\n",
    "    bigram_c = defaultdict(int)\n",
    "    trigram_c = defaultdict(int)\n",
    "    if lvl!=\"word\":\n",
    "        for sent in corpus:\n",
    "            for word in sent:\n",
    "                \n",
    "                unigrams=calc_ngrams(list(word),1)\n",
    "                bigrams=calc_ngrams(list(word),2)\n",
    "                trigrams=calc_ngrams(list(word),3)\n",
    "                \n",
    "                for unigram in unigrams:\n",
    "                    unigram_c[unigram] += 1\n",
    "\n",
    "                for bigram in bigrams:\n",
    "                    bigram_c[bigram] += 1\n",
    "\n",
    "                for trigram in trigrams:\n",
    "                    trigram_c[trigram] += 1\n",
    "\n",
    "    else:    \n",
    "        for sent in corpus:\n",
    "            unigrams = calc_ngrams(sent, 1)\n",
    "            bigrams = calc_ngrams(sent, 2)\n",
    "            trigrams = calc_ngrams(sent, 3)\n",
    "\n",
    "            for unigram in unigrams:\n",
    "                unigram_c[unigram] += 1\n",
    "\n",
    "            for bigram in bigrams:\n",
    "                bigram_c[bigram] += 1\n",
    "\n",
    "            for trigram in trigrams:\n",
    "                trigram_c[trigram] += 1\n",
    "\n",
    "    return unigram_c, bigram_c, trigram_c\n",
    "\n",
    "def ml_prob(ngram,counts): ## ngram :- [w1,w2,w3]\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        if ngram in unigrams:\n",
    "            prob=unigrams[ngram]/float(sum(unigrams.values()))\n",
    "        else:\n",
    "            prob=0\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        w1=ngram[0]\n",
    "        w2=ngram[1]\n",
    "        if ngram in bigrams:\n",
    "            prob = bigrams[ngram]/float(unigrams[ngram[0:1]])\n",
    "        else:\n",
    "            prob=0\n",
    "    else:\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        if ngram in trigrams:\n",
    "            prob=trigrams[ngram]/float(bigrams[ngram[0:2]])\n",
    "        else:\n",
    "            prob=0\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T19:33:32.433779Z",
     "start_time": "2018-12-27T19:15:58.692738Z"
    }
   },
   "outputs": [],
   "source": [
    "#word_uni,word_bi,word_tri = calc_MLE_counts(corpus)\n",
    "#ch_uni,ch_bi,ch_tri = calc_MLE_counts(corpus,\"char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving word and character MLE counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T19:34:37.751317Z",
     "start_time": "2018-12-27T19:33:32.436371Z"
    }
   },
   "outputs": [],
   "source": [
    "'''file = open(\"word_uni.pickle\",\"wb\")\n",
    "pickle.dump(word_uni,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_bi.pickle\",\"wb\")\n",
    "pickle.dump(word_bi,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_tri.pickle\",\"wb\")\n",
    "pickle.dump(word_tri,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_uni.pickle\",\"wb\")\n",
    "pickle.dump(ch_uni,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_bi.pickle\",\"wb\")\n",
    "pickle.dump(ch_bi,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_tri.pickle\",\"wb\")\n",
    "pickle.dump(ch_tri,file)\n",
    "file.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word and character MLE counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T19:35:26.815678Z",
     "start_time": "2018-12-27T19:34:37.753795Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(\"word_uni.pickle\",\"rb\")\n",
    "word_uni = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_bi.pickle\",\"rb\")\n",
    "word_bi = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_tri.pickle\",\"rb\")\n",
    "word_tri = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_uni.pickle\",\"rb\")\n",
    "ch_uni = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_bi.pickle\",\"rb\")\n",
    "ch_bi = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"ch_tri.pickle\",\"rb\")\n",
    "ch_tri = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T19:35:26.861047Z",
     "start_time": "2018-12-27T19:35:26.818135Z"
    }
   },
   "outputs": [],
   "source": [
    "def laplace_smoothing(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        N=sum(unigrams.values())\n",
    "        V=len(unigrams)\n",
    "        if ngram in unigrams:\n",
    "            prob = (unigrams[ngram]+1)/float(N+V)\n",
    "        else:\n",
    "            prob = 1/float(N+V)\n",
    "\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        V=len(unigrams)\n",
    "        if ngram in bigrams:\n",
    "            prob = (bigrams[ngram]+1)/float(unigrams[ngram[0:1]]+V)\n",
    "        else:\n",
    "            if ngram[0:1] in unigrams:\n",
    "                prob = 1/float(unigrams[ngram[0:1]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        V=len(unigrams)\n",
    "        if ngram in trigrams:\n",
    "            prob = (trigrams[ngram]+1)/float(bigrams[ngram[0:2]]+V)\n",
    "        else:\n",
    "            if ngram[0:2] in bigrams:\n",
    "                prob = 1/float(bigrams[ngram[0:2]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    return prob\n",
    "\n",
    "def good_turing(ngram,counts):\n",
    "    V=sum(counts[0].values())\n",
    "    ngram=tuple(ngram)\n",
    "    l=len(ngram)\n",
    "    ngrams=counts[l-1]\n",
    "    N=sum(ngrams.values())\n",
    "    nrs={}\n",
    "    for k,v in ngrams.items():\n",
    "        if v not in nrs:\n",
    "            nrs[v].append(k) ## dictionary of (key=freq of ngram) and (value = list of all ngram tuple whose freq. is key)\n",
    "    nr_counts={k:len(v) for k,v in nrs.items()}\n",
    "    if 0 not in nr_counts:\n",
    "        nr_counts[0]=V**l - N\n",
    "    else:\n",
    "        nr_counts[0]+=V**l - N\n",
    "    MAX=sorted(nr_counts.items())[0][1] ##max freq of ngram in corpus\n",
    "    new_nrs={}\n",
    "    for r, nr in nr_counts.items():\n",
    "        if (r+1) in nr_counts:\n",
    "            new_nr=(r+1)*nr_counts[r+1]/float(N)\n",
    "        else:\n",
    "            new_nr=MAX*r**-2/float(N)\n",
    "        new_nrs[r]=new_nr\n",
    "\n",
    "    if ngrams[ngram]>5:\n",
    "        prob=ml_prob(ngram,counts)\n",
    "    else:\n",
    "        denominator=(1 - 6*new_nrs[6]/float(new_nrs[1]))/float(N)\n",
    "        if ngram in ngrams:\n",
    "            numerator=(ngrams[ngram]+1)*new_nrs[ngrams[ngram]+1]/float(new_nrs[ngrams[ngram]])\n",
    "            mod_num=num - ngrams[ngram]*6*new_nrs[6]/float(new_nrs[1])\n",
    "            prob = mod_num/float(denominator)\n",
    "        else:\n",
    "            prob = new_nrs[1]/(new_nrs[0]*float(denominator)) ##not considering singleton ngrams as unseen\n",
    "    return prob\n",
    "\n",
    "def witten_bell(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        ngram_count=bigrams[ngram]\n",
    "        prior_count=unigrams[ngram[0:1]]\n",
    "        type_count=0\n",
    "        s=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[0:1]:\n",
    "                type_count+=1\n",
    "                s+=bigrams[bigram]\n",
    "        vocab_size=len(bigrams)\n",
    "        z = vocab_size - type_count\n",
    "        if ngram_count==0:\n",
    "            prob = type_count/float(z*(prior_count+type_count))\n",
    "        else:\n",
    "            prob = ngram_count/float(prior_count + type_count)\n",
    "        wb_lambda=1-bigrams[bigram]/float(bigrams[bigram]+s)\n",
    "        prob=(wb_lambda)*prob+(1-wb_lambda)*unigrams[ngram[1:2]]/float(sum(unigrams.values()))\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        ngram_count = trigrams[ngram]\n",
    "        prior_count = bigrams[ngram[0:2]]\n",
    "        type_count=0\n",
    "        s=0\n",
    "        for trigram in trigrams:\n",
    "            if ngram[0:1]==trigram[0] and ngram[1:2]==trigram[1]:\n",
    "                type_count+=1\n",
    "                s+=trigrams[trigram]\n",
    "        vocab_size = len(trigrams)\n",
    "        z = vocab_size - type_count\n",
    "        if ngram_count == 0:\n",
    "            prob = type_count/float(z*(prior_count + type_count))\n",
    "        else:\n",
    "            prob = ngram_count/float(prior_count + type_count)\n",
    "        wb_lambda=1-trigrams[trigram]/float(trigrams[trigram]+s)\n",
    "        prob=(wb_lambda)*prob+(1-wb_lambda)*bigrams[ngram[1:3]]/float(unigrams[ngram[1:2]])\n",
    "    return prob\n",
    "\n",
    "def stupid_backoff(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    ans=0\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        if ngram in bigrams:\n",
    "            denominator=0\n",
    "            for bigram in bigrams:\n",
    "                if ngram[0:1]==bigram[0]:\n",
    "                    denominator+=bigrams[bigram]\n",
    "            ans=bigram[ngram]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[1:2] in unigrams:\n",
    "                ans = 0.4 * unigrams[ngram[1:2]]/float(sum(unigrams.values()))\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        if ngram in trigrams:\n",
    "            denominator=0\n",
    "            for trigram in trigrams:\n",
    "                if ngram[0:1]==trigram[0] and ngram[1:2]==trigram[1]:\n",
    "                    denominator+=trigrams[trigram]\n",
    "            ans=trigrams[ngram]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[1:3] in bigrams:\n",
    "                denominator=0\n",
    "                for bigram in bigrams:\n",
    "                    if ngram[1:2]==bigram[0]:\n",
    "                        denominator+=bigrams[bigram]\n",
    "                ans=0.4*bigrams[ngram[1:3]]/float(denominator)\n",
    "        if ans==0:\n",
    "            if ngram[2:3] in unigrams:\n",
    "                ans = 0.16*unigrams[ngram[2:3]]/float(sum(unigrams.values()))\n",
    "    return ans\n",
    "\n",
    "def katz_backoff(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        r = bigrams[ngram]\n",
    "        k=5\n",
    "        ngrams=bigrams\n",
    "        N=sum(ngrams.values())\n",
    "        nrs={}\n",
    "        for k,v in ngrams.items():\n",
    "            if v not in nrs:\n",
    "                nrs[v].append(k) ## dictionary of key as freq of ngram and value as list of all ngram tuple\n",
    "        nr_counts={k:len(v) for k,v in nrs.items()}\n",
    "        MAX=sorted(nr_counts.items())[0][1] ##max freq of ngram in corpus\n",
    "        new_nrs={}\n",
    "        for r, nr in nr_counts.items():\n",
    "            if (r+1) in nr_counts:\n",
    "                new_nr=(r+1)*nr_counts[r+1]/float(N)\n",
    "            else:\n",
    "                new_nr=MAX*r**-2/float(N)\n",
    "            new_nrs[r]=new_nr\n",
    "        num1 = (r+1)*new_nrs[r+1]/float(r*new_nrs[r])\n",
    "        num2 = (k+1)*new_nrs[k+1]/float(new_nrs[1])\n",
    "        deno =  1 - (k+1)*new_nrs[k+1]/float(new_nrs[1])\n",
    "        dr = (num1 - num2)/float(deno)\n",
    "        s=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[0:1]:\n",
    "                s+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        numerator=1-s\n",
    "        s=0\n",
    "        for unigram in unigrams:\n",
    "            s+=good_turing([unigram[0]],counts)\n",
    "        denominator=1-s\n",
    "        alpha = numerator/float(denominator)\n",
    "        if r>k:\n",
    "            prob = good_turing([ngram[0:1][0],ngram[1:2][0]],counts)\n",
    "        elif r>0 and r<=k:\n",
    "            prob = dr*good_turing([ngram[0:1][0],ngram[1:2][0]],counts)\n",
    "        else:\n",
    "            prob = alpha*good_turing([ngram[1:2][0]],counts)\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        r1 = trigrams[ngram]\n",
    "        r2= bigrams[ngram[1:3]]\n",
    "        s1=0\n",
    "        for trigram in trigrams:\n",
    "            if trigram[0]==ngram[0:1] and trigram[1]==ngram[1:2]:\n",
    "                s1+=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "        num=1-s1\n",
    "        s1=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[1:2]:\n",
    "                s1+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        deno=1-s1\n",
    "        alpha1=num/float(deno)\n",
    "        s2=0\n",
    "        for bigram in bigrams:\n",
    "            if bigram[0]==ngram[1:2]:\n",
    "                s+=good_turing([bigram[0],bigram[1]],counts)\n",
    "        numerator=1-s\n",
    "        s2=0\n",
    "        for unigram in unigrams:\n",
    "            s2+=good_turing([unigram[0]],counts)\n",
    "        denominator=1-s\n",
    "        alpha2 = numerator/float(denominator)\n",
    "        if r1>0:\n",
    "            prob=good_turing([ngram[0:1][0],ngram[1:2][0],ngram[2:3][0]],counts)\n",
    "        elif r1==0 and r2>0:\n",
    "            prob=alpha1*good_turing([ngram[1:2][0],ngram[2:3][0]],counts)\n",
    "        else:\n",
    "            prob=alpha2*good_turing([ngram[2:3][0]],counts)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T19:35:26.887505Z",
     "start_time": "2018-12-27T19:35:26.863038Z"
    }
   },
   "outputs": [],
   "source": [
    "def language_model(model_name,counts,n):\n",
    "    ##model_name :- \"lap\",\"gt\",\"wb\", \"sbo\", \"kbo\" [laplace/add-1,good-turing,witten_bell,stupid_backoff,katz-backoff]\n",
    "    unigrams=counts[0]\n",
    "    bigrams=counts[1]\n",
    "    trigrams=counts[2]\n",
    "    unigram_prob=defaultdict(float)\n",
    "    bigram_prob=defaultdict(float)\n",
    "    trigram_prob=defaultdict(float)\n",
    "    if n==1:\n",
    "        for unigram in unigrams:\n",
    "            if model_name==\"lap\":\n",
    "                unigram_prob[unigram]=laplace_smoothing([unigram[0]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                unigram_prob[unigram]=good_turing([unigram[0]],counts)\n",
    "        return unigram_prob\n",
    "\n",
    "    elif n==2:\n",
    "        for bigram in bigrams:\n",
    "            if model_name==\"lap\":\n",
    "                bigram_prob[bigram]=laplace_smoothing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                bigram_prob[bigram]=good_turing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                bigram_prob[bigram]=witten_bell([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                bigram_prob[bigram]=stupid_backoff([bigram[0],bigram[1]],counts)\n",
    "            else:\n",
    "                bigram_prob[bigram]=katz_backoff([bigram[0],bigram[1]],counts)\n",
    "        return bigram_prob\n",
    "\n",
    "    else:\n",
    "        for trigram in trigrams:\n",
    "            if model_name==\"lap\":\n",
    "                trigram_prob[trigram]=laplace_smoothing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                trigram_prob[trigram]=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                trigram_prob[trigram]=witten_bell([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                trigram_prob[trigram]=stupid_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            else:\n",
    "                trigram_prob[trigram]=katz_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "        return trigram_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T21:14:19.219120Z",
     "start_time": "2018-12-27T19:37:40.978321Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3eccde70de9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_uni_lap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lap\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_uni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_bi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_tri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_uni_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_uni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_bi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_tri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword_bi_lap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lap\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_uni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_bi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_tri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_bi_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_uni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_bi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_tri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2f61d67b0435>\u001b[0m in \u001b[0;36mlanguage_model\u001b[0;34m(model_name, counts, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0munigram_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munigram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlaplace_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"gt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0munigram_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munigram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgood_turing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munigram_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-dbf4c1433b41>\u001b[0m in \u001b[0;36mgood_turing\u001b[0;34m(ngram, counts)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mnrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dictionary of (key=freq of ngram) and (value = list of all ngram tuple whose freq. is key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mnr_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnr_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "word_uni_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],1)\n",
    "word_uni_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],1)\n",
    "\n",
    "word_bi_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_wb = language_model(\"wb\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_sbo = language_model(\"sbo\",[word_uni,word_bi,word_tri],2)\n",
    "word_bi_kbo = language_model(\"kbo\",[word_uni,word_bi,word_tri],2)\n",
    "\n",
    "word_tri_lap = language_model(\"lap\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_gt = language_model(\"gt\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_wb = language_model(\"wb\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_sbo = language_model(\"sbo\",[word_uni,word_bi,word_tri],3)\n",
    "word_tri_kbo = language_model(\"kbo\",[word_uni,word_bi,word_tri],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"word_uni_lap.pickle\",\"wb\")\n",
    "pickle.dump(word_uni_lap,file)\n",
    "file.close()\n",
    "file = open(\"word_uni_gt.pickle\",\"wb\")\n",
    "pickle.dump(word_uni_gt,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_bi_lap.pickle\",\"wb\")\n",
    "pickle.dump(word_bi_lap,file)\n",
    "file.close()\n",
    "file = open(\"word_bi_gt.pickle\",\"wb\")\n",
    "pickle.dump(word_bi_gt,file)\n",
    "file.close()\n",
    "file = open(\"word_bi_wb.pickle\",\"wb\")\n",
    "pickle.dump(word_bi_wb,file)\n",
    "file.close()\n",
    "file = open(\"word_bi_sbo.pickle\",\"wb\")\n",
    "pickle.dump(word_bi_sbo,file)\n",
    "file.close()\n",
    "file = open(\"word_bi_kbo.pickle\",\"wb\")\n",
    "pickle.dump(word_bi_kbo,file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_tri_lap.pickle\",\"wb\")\n",
    "pickle.dump(word_tri_lap,file)\n",
    "file.close()\n",
    "file = open(\"word_tri_gt.pickle\",\"wb\")\n",
    "pickle.dump(word_tri_gt,file)\n",
    "file.close()\n",
    "file = open(\"word_tri_wb.pickle\",\"wb\")\n",
    "pickle.dump(word_tri_wb,file)\n",
    "file.close()\n",
    "file = open(\"word_tri_sbo.pickle\",\"wb\")\n",
    "pickle.dump(word_tri_sbo,file)\n",
    "file.close()\n",
    "file = open(\"word_tri_kbo.pickle\",\"wb\")\n",
    "pickle.dump(word_tri_kbo,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"word_uni_lap.pickle\",\"rb\")\n",
    "word_uni_lap = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_uni_gt.pickle\",\"rb\")\n",
    "word_uni_gt = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_bi_lap.pickle\",\"rb\")\n",
    "word_bi_lap = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_bi_gt.pickle\",\"rb\")\n",
    "word_bi_gt = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_bi_wb.pickle\",\"rb\")\n",
    "word_bi_wb = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_bi_sbo.pickle\",\"rb\")\n",
    "word_bi_sbo = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_bi_kbo.pickle\",\"rb\")\n",
    "word_bi_kbo = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"word_tri_lap.pickle\",\"rb\")\n",
    "word_tri_lap = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_tri_gt.pickle\",\"rb\")\n",
    "word_tri_gt = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_tri_wb.pickle\",\"rb\")\n",
    "word_tri_wb = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_tri_sbo.pickle\",\"rb\")\n",
    "word_tri_sbo = pickle.load(file)\n",
    "file.close()\n",
    "file = open(\"word_tri_kbo.pickle\",\"rb\")\n",
    "word_tri_kbo = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find next word after given N-1 grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.656499Z",
     "start_time": "2018-11-03T14:20:16.270Z"
    }
   },
   "outputs": [],
   "source": [
    "def complete_ngram(ngram,counts,smoothing_method): ##ngram = [w1,w2] or [w1]\n",
    "# Given (N-1) gram, and the value 'N', print the possibilities that complete the n-gram\n",
    "# and plot them in decresing order of frequency\n",
    "    next_gram=[]\n",
    "    unigrams=counts[0]\n",
    "    for word in unigrams:\n",
    "        if len(ngram)==2:\n",
    "            tokens=[ngram[0],ngram[1],word[0]]\n",
    "        else:\n",
    "            tokens=[ngram[0],word[0]]\n",
    "        if smoothing_method==\"lap\":\n",
    "            prob = laplace_smoothing(tokens,counts)\n",
    "        elif smoothing_method==\"gt\":\n",
    "            prob = good_turing(tokens,counts)\n",
    "        elif smoothing_method==\"wb\":\n",
    "            prob = witten_bell(tokens,counts)\n",
    "        elif smoothing_method==\"sbo\":\n",
    "            prob=stupid_backoff(tokens,counts)\n",
    "        else:\n",
    "            prob=katz_backoff(token,counts)\n",
    "        next_gram.append((prob,word[0]))\n",
    "    next_gram.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    return next_gram[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.657412Z",
     "start_time": "2018-11-03T14:20:16.271Z"
    }
   },
   "outputs": [],
   "source": [
    "inp=\"The man\"\n",
    "ngram=inp.split()\n",
    "counts=[word_uni,word_bi,word_tri]\n",
    "smoothing_method=[\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\"]\n",
    "for smoothing in smoothing_method:\n",
    "    print(\"Smoothing: \",smoothing)\n",
    "    print(complete_ngram(ngram,counts,smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T23:04:40.546247Z",
     "start_time": "2018-11-02T23:04:40.531326Z"
    }
   },
   "source": [
    "# Edit Distance and candidates words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.658291Z",
     "start_time": "2018-11-03T14:20:16.275Z"
    }
   },
   "outputs": [],
   "source": [
    "def edit_distance(word,letters):\n",
    "    #letters    = 'abcdefghijklmnopqrstuvwxyz' ##add additional characters\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def find_candidates(word,unique_tokens):\n",
    "    ##Fusion words can also be computed in order to enhance the list\n",
    "    letters=[]\n",
    "    for token in unique_tokens:\n",
    "        for ch in list(token):\n",
    "            if ch not in letters:\n",
    "                letters.append(ch)\n",
    "    letters=\"\".join(str(ch) for ch in letters)\n",
    "    c4=set(word)\n",
    "    if word in unique_tokens:\n",
    "        c1=set(word)\n",
    "    edit1=edit_distance(word,letters)\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "    for w1 in edit1:\n",
    "        for w2 in edit_distance(w1,letters):\n",
    "            if w2 in unique_tokens:\n",
    "                tmp2.append(w2)\n",
    "        if w1 in unique_tokens:\n",
    "            tmp1.append(w1)\n",
    "    c2=set(tmp1)\n",
    "    c3=set(temp2)\n",
    "    return list(c1|c2|c3|c4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.659242Z",
     "start_time": "2018-11-03T14:20:16.279Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def spell_checker(word,n,model_name,counts,unique_tokens):\n",
    "# Given a word check if the spelling is correct using your Language model\n",
    "# Feel free to add more helper functions for this task\n",
    "# Giving candidate word with maximum probability using character L.M\n",
    "# word level L.M is not computed and comapred against character level which is left for future task\n",
    "    candidates=find_candidates(word,unique_tokens)\n",
    "    cand_probs=[]\n",
    "    for cand in candidates:\n",
    "        cand_ch_grams=[cand[i:i+n] for i in range(len(cand)-n+1)]\n",
    "        if n==1:\n",
    "            prod=1\n",
    "            for unigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([unigram],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([unigram],counts)\n",
    "        elif n==2:\n",
    "            prod=1\n",
    "            first_ch = list(cand_ch_grams[0])[0]\n",
    "            prod*=good_turing([first_ch],counts)\n",
    "            for bigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"wb\":\n",
    "                    prod*=witten_bell([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                elif model_name==\"sbo\":\n",
    "                    prod*=stupid_backoff([list(bigram)[0],list(bigram)[1]],counts)\n",
    "                else:\n",
    "                    prod*=katz_backoff([list(bigram)[0],list(bigram)[1]],counts)\n",
    "\n",
    "        else:\n",
    "            prod=1\n",
    "            first_ch=list(cand_ch_grams[0])[0]\n",
    "            second_ch=list(cand_ch_grams[0])[1]\n",
    "            prod*=good_turing([first_ch],counts)\n",
    "            prod*=good_turing([first_ch,second_ch],counts)\n",
    "            for trigram in cand_ch_grams:\n",
    "                if model_name==\"lap\":\n",
    "                    prod*=laplace_smoothing([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"gt\":\n",
    "                    prod*=good_turing([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"wb\":\n",
    "                    prod*=witten_bell([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                elif model_name==\"sbo\":\n",
    "                    prod*=stupid_backoff([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "                else:\n",
    "                    prod*=katz_backoff([list(trigram)[0],list(trigram)[1],list(trigram)[2]],counts)\n",
    "\n",
    "        cand_probs.append((prob,cand))\n",
    "    cand_probs.sort(key=lambda tup: tup[0], reverse = True)\n",
    "    return cand_probs[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.660108Z",
     "start_time": "2018-11-03T14:20:16.281Z"
    }
   },
   "outputs": [],
   "source": [
    "input_word=\"catsq\"\n",
    "model_comb=[(\"lap\",\"gt\"),(\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\")]\n",
    "ns=[1,2,3]\n",
    "counts=[ch_uni,ch_bi,ch_tri]\n",
    "for n in ns:\n",
    "    print(\"N: \\n\",n)\n",
    "    if n==1:\n",
    "        model_names = model_comb[0]\n",
    "    else:\n",
    "        model_names = model_comb[1]\n",
    "    for model in model_names:\n",
    "        print(\"Smoothing: \",model)\n",
    "        print(spell_checker(input_word,n,model,counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammaticality of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.661045Z",
     "start_time": "2018-11-03T14:20:16.285Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_grammaticality(sentence,n,model_name,counts):\n",
    "  # Given a sentence, Build a model from the data which can give a score of grammaticality.\n",
    "  # More grammatical the sentence, better the score\n",
    "  # Feel free to add helper functions\n",
    "    n_grams = list(zip(*[sentence.split()[i:] for i in range(n)]))\n",
    "    if n==1:\n",
    "        prod=1\n",
    "        for unigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([unigram[0]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([unigram[0]],counts)\n",
    "    elif n==2:\n",
    "        prod=1\n",
    "        first_word = n_grams[0][0]\n",
    "        prod*=good_turing([first_word],counts)\n",
    "        for bigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                prod*=witten_bell([bigram[0],bigram[1]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                prod*=stupid_backoff([bigram[0],bigram[1]],counts)\n",
    "            else:\n",
    "                prod*=katz_backoff([bigram[0],bigram[1]],counts)\n",
    "\n",
    "    else:\n",
    "        prod=1\n",
    "        first_word= n_grams[0][0]\n",
    "        second_word=ngrams[0][1]\n",
    "        prod*=good_turing([first_word],counts)\n",
    "        prod*=good_turing([first_word,second_word],counts)\n",
    "        for trigram in n_grams:\n",
    "            if model_name==\"lap\":\n",
    "                prod*=laplace_smoothing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"gt\":\n",
    "                prod*=good_turing([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"wb\":\n",
    "                prod*=witten_bell([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            elif model_name==\"sbo\":\n",
    "                prod*=stupid_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "            else:\n",
    "                prod*=katz_backoff([trigram[0],trigram[1],trigram[2]],counts)\n",
    "\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T14:21:36.661915Z",
     "start_time": "2018-11-03T14:20:16.287Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1='I have a red apple'\n",
    "sentence2='apple a have I red'\n",
    "print(\"Sentence1 :\",sentence1)\n",
    "print(\"Sentence2 :\",sentence2)\n",
    "\n",
    "model_comb=[(\"lap\",\"gt\"),(\"lap\",\"gt\",\"wb\",\"sbo\",\"kbo\")]\n",
    "ns=[1,2,3]\n",
    "counts=[word_uni,word_bi,word_tri]\n",
    "for n in ns:\n",
    "    print(\"N: \\n\",n)\n",
    "    if n==1:\n",
    "        model_names = model_comb[0]\n",
    "    else:\n",
    "        model_names = model_comb[1]\n",
    "    for model in model_names:\n",
    "        print(\"Smoothing: \",model)\n",
    "        print(\"Sentence1 Score: \",score_grammaticality(sentence1,n,model,counts))\n",
    "        print(\"Sentence2 Score: \",score_grammaticality(sentence2,n,model,counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
