{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    return Counter(zip(*[text[i:] for i in range(n)]))\n",
    "\n",
    "def get_ngrams(n,tokens):\n",
    "    ngrams = find_ngrams(tokens,n)\n",
    "    return ngrams\n",
    "\n",
    "def ml_prob(ngram,counts): ## ngram :- [(w1,cl1),(w2,cl2),(w3,cl3)]\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        if ngram in unigrams:\n",
    "            prob=unigrams[ngram]/float(sum(unigrams.values()))\n",
    "        else:\n",
    "            prob=0\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        w1=ngram[0]\n",
    "        w2=ngram[1]\n",
    "        if ngram in bigrams:\n",
    "            prob = bigrams[ngram]/float(unigrams[ngram[0:1]])\n",
    "        else:\n",
    "            prob=0\n",
    "    else:\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[0]\n",
    "        if ngram in trigrams:\n",
    "            prob=trigrams[ngram]/float(bigrams[ngram[0:2]])\n",
    "        else:\n",
    "            prob=0\n",
    "    return prob\n",
    "\n",
    "def laplace_smoothing(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    if len(ngram)==1:\n",
    "        unigrams=counts[0]\n",
    "        N=sum(unigrams.values())\n",
    "        V=len(unigrams)\n",
    "        if ngram in unigrams:\n",
    "            prob = (unigrams[ngram]+1)/float(N+V)\n",
    "        else:\n",
    "            prob = 1/float(N+V)\n",
    "\n",
    "    elif len(ngram)==2:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        V=len(unigrams)\n",
    "        if ngram in bigrams:\n",
    "            prob = (bigrams[ngram]+1)/float(unigrams[ngram[0:1]]+V)\n",
    "        else:\n",
    "            if ngram[0:1] in unigrams:\n",
    "                prob = 1/float(unigrams[ngram[0:1]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    else:\n",
    "        unigrams=counts[0]\n",
    "        bigrams=counts[1]\n",
    "        trigrams=counts[2]\n",
    "        V=len(unigrams)\n",
    "        if ngram in trigrams:\n",
    "            prob = (trigrams[ngram]+1)/float(bigrams[ngram[0:2]]+V)\n",
    "        else:\n",
    "            if ngram[0:2] in bigrams:\n",
    "                prob = 1/float(bigrams[ngram[0:2]]+V)\n",
    "            else:\n",
    "                prob = 1/float(V)\n",
    "    return prob\n",
    "\n",
    "def good_turing(ngram,counts):\n",
    "    ngram=tuple(ngram)\n",
    "    l=len(ngram)\n",
    "    ngrams=counts[l-1]\n",
    "    N=sum(ngrams.values())\n",
    "    nrs={}\n",
    "    for k,v in ngrams.items():\n",
    "        if v not in nrs:\n",
    "            nrs[v].append(k) ## dictionary of (key=freq of ngram) and (value = list of all ngram tuple whose freq. is key)\n",
    "    nr_counts={k:len(v) for k,v in nrs.items()}\n",
    "    MAX=sorted(nr_counts.items())[0][1] ##max freq of ngram in corpus\n",
    "    new_nrs={}\n",
    "    for r, nr in nr_counts.items():\n",
    "        if (r+1) in nr_counts:\n",
    "            new_nr=(r+1)*nr_counts[r+1]/float(N)\n",
    "        else:\n",
    "            new_nr=MAX*r**-2/float(N)\n",
    "        new_nrs[r]=new_nr\n",
    "\n",
    "    if ngrams[ngram]>5:\n",
    "        prob=ml_prob(ngram,counts)\n",
    "    else:\n",
    "        denominator=(1 - 6*new_nrs[6]/float(new_nrs[1]))/float(N)\n",
    "        if ngram in ngrams:\n",
    "            numerator=(ngrams[ngram]+1)*new_nrs[ngrams[ngram]+1]/float(new_nrs[ngrams[ngram]])\n",
    "            mod_num=num - ngrams[ngram]*6*new_nrs[6]/float(new_nrs[1])\n",
    "            prob = mod_num/float(denominator)\n",
    "        else:\n",
    "            prob = new_nrs[1]/float(denominator) ##not considering singleton ngrams as unseen\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_argmax(l):\n",
    "    ##list of tuple :- [(cl,prob),....]\n",
    "    return sorted(l, key=lambda x: float(x[1]), reverse=True)[0][0]\n",
    "\n",
    "def P_cl(err,unigrams):\n",
    "\tcnt=0\n",
    "\tfor uni in unigrams:\n",
    "\t\tif uni[0][1]==err:\n",
    "\t\t\tcnt+=1\n",
    "\tT=sum(unigrams.values())\n",
    "\treturn cnt/T\n",
    "\n",
    "def first_word_error(first_word,errors,counts,model_name=\"gt\"):\n",
    "\tunigrams=counts[0]\n",
    "\terr_probs=[]\n",
    "\tfor err in errors:\n",
    "\t\tunigram=[(first_word,err)]\n",
    "\t\tif model_name==\"lap\":\n",
    "\t\t\tprob = laplace_smoothing(unigram,counts)\n",
    "\t\telse:\n",
    "\t\t\tprob = good_turing(unigram,counts)\n",
    "\t\tpcl = P_cl(err,unigrams)\n",
    "\t\terr_probs.append((err,prob*pcl))\n",
    "\treturn find_argmax(err_probs)\n",
    "\n",
    "def second_word_error(second_word,first_word,first_word_err,errors,counts,model_name=\"gt\"):\n",
    "\terr_probs=[]\n",
    "\tunigrams=counts[0]\n",
    "\tfor err in errors:\n",
    "\t\tbigram=[(first_word,first_word_err),(second_word,err)]\n",
    "\t\tif model_name==\"lap\":\n",
    "\t\t\tprob = laplace_smoothing(bigram,counts)\n",
    "\t\telse:\n",
    "\t\t\tprob = good_turing(bigram,counts)\n",
    "\t\tpcl = P_cl(err,unigrams)\n",
    "\t\terr_probs.append((err,prob*pcl))\n",
    "\treturn find_argmax(err_probs)\n",
    "\n",
    "def compute_error(te,errors,counts,model_name):\n",
    "\tdef compute_ngrams(text,n):\n",
    "\t\treturn zip(*[text[i:] for i in range(n)])\n",
    "\tunigrams=counts[0]\n",
    "\tbigrams=counts[1]\n",
    "\ttrigrams=counts[2]\n",
    "\tERRORS=[]\n",
    "\tfor sentence in te:\n",
    "\t\ttokens=sentence.split(\"\\n\")\n",
    "\t\tsent=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\ttoken_list=token.strip().split()\n",
    "\t\t\tif len(token_list)!=0:\n",
    "\t\t\t\tsent.append(token_list[0].lower())\n",
    "\t\tfirst_word,second_word=sent[0],sent[1]\n",
    "\t\tfirst_word_err=first_word_error(first_word,errors,counts,model_name)\n",
    "\t\tsecond_word_err=second_word_error(second_word,first_word,first_word_err,errors,counts,model_name)\n",
    "\t\ttemp=[]\n",
    "\t\tc1=first_word_err\n",
    "\t\tc2=second_word_err\n",
    "\t\ttemp.append(c1)\n",
    "\t\ttemp.append(c2)\n",
    "\t\tfor ngram in compute_ngrams(sent,3):\n",
    "\t\t\terr_probs=[]\n",
    "\t\t\tfor err in errors:\n",
    "\t\t\t\tc3=err\n",
    "\t\t\t\ttrigram=[(ngram[0],c1),(ngram[1],c2),(ngram[2],c3)]\n",
    "\t\t\t\tif model_name==\"lap\":\n",
    "\t\t\t\t\tprob = laplace_smoothing(trigram,counts)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprob = good_turing(trigram,counts)\n",
    "\t\t\t\tpcl=P_cl(err,unigrams)\n",
    "\t\t\t\terr_probs.append((err,prob*pcl))\n",
    "\t\t\tc1=c2\n",
    "\t\t\tc2=find_argmax(err_probs)\n",
    "\t\t\ttemp.append(c2)\n",
    "\t\tERRORS.append(temp)\n",
    "\treturn ERRORS\n",
    "\n",
    "\n",
    "\n",
    "def find_scores(te,correct_results,errors,counts,model_name):\n",
    "\tERRORS = compute_error(te,errors,counts,model_name)\n",
    "\ti=0\n",
    "\tacc_sent_level=0\n",
    "\tacc_word_level=0\n",
    "\ttotal_words = sum(len(l) for l in ERRORS)\n",
    "\tfor sentence in correct_results:\n",
    "\t\ttokens=sentence.split(\"\\n\")\n",
    "\t\tsent_errors=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\ttoken_list=token.strip().split()\n",
    "\t\t\tif len(token_list)!=0:\n",
    "\t\t\t\tif len(token_list)==1:\n",
    "\t\t\t\t\tsent_errors.append(\"no-error\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsent_errors.append(token_list[len(token_list)-1])\n",
    "\t\tacc_word_level+=len(list(set(ERRORS[i] & set(sent_errors))))\n",
    "\n",
    "\t\tif len(sent_errors)==len(list(set(ERRORS[i] & set(sent_errors)))):\n",
    "\t\t\tacc_sent_level+=1\n",
    "\t\ti+=1\n",
    "\tacc_word_level=acc_word_level/total_words\n",
    "\tacc_sent_level=acc_sent_level/len(correct_results)\n",
    "\tprint(acc_word_level,acc_sent_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\tscript_path=os.path.dirname(os.path.abspath(__file__))\n",
    "\tdata_path=script_path+\"/Data\"\n",
    "\ttrain_data=data_path+\"/train.txt\"\n",
    "\ttest_data=data_path+\"/dev.txt\"\n",
    "\tcorrect_data=data_path+\"/dev_results.txt\"\n",
    "\ttr=open(train_data).read().split(\"\\n\\n\")\n",
    "\tte=open(test_data).read().split(\"\\n\\n\")\n",
    "\tcorrect_results=open(correct_data).read().split(\"\\n\\n\")\n",
    "\tword_uni=Counter()\n",
    "\tword_bi=Counter()\n",
    "\tword_tri=Counter()\n",
    "\terrors=[\"no-error\"]\n",
    "\tfor sentence in tr:\n",
    "\t\ttokens=sentence.split(\"\\n\")\n",
    "\t\tsent=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\ttoken_list=token.strip().split()\n",
    "\t\t\tif len(token_list)!=0:\n",
    "\t\t\t\tif len(token_list)==1:\n",
    "\t\t\t\t\tsent.append((token_list[0].lower(),\"no-error\"))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsent.append((token_list[0].lower(),token_list[len(token_list)-1]))\n",
    "\t\t\t\t\tif token_list[len(token_list)-1] not in errors:\n",
    "\t\t\t\t\t\terrors.append(token_list[len(token_list)-1])\n",
    "\t\tword_uni+=get_ngrams(1,sent)\n",
    "\t\tword_bi+=get_ngrams(2,sent)\n",
    "\t\tword_tri+=get_ngrams(3,sent)\n",
    "\t'''with open(script_path+\"/saved_items/word_uni.pickle\",'wb') as fl:\n",
    "\t\tpickle.dump(word_uni,fl)\n",
    "\tfl.close()\n",
    "\twith open(script_path+\"/saved_items/word_bi.pickle\",'wb') as fl:\n",
    "        \tpickle.dump(word_bi,fl)\n",
    "\tfl.close()\n",
    "\twith open(script_path+\"/saved_items/word_tri.pickle\",'wb') as fl:\n",
    "\t\tpickle.dump(word_tri,fl)\n",
    "\tfl.close()'''\n",
    "\tfl = open(script_path+\"/saved_items/word_uni.pickle\",'rb')\n",
    "\tword_uni=pickle.load(fl)\n",
    "\tfl.close()\n",
    "\tfl = open(script_path+\"/saved_items/word_bi.pickle\",'rb')\n",
    "\tword_bi = pickle.load(fl)\n",
    "\tfl.close()\n",
    "\tfl = open(script_path+\"/saved_items/word_tri.pickle\",'rb')\n",
    "\tword_tri = pickle.load(fl)\n",
    "\tfl.close()\n",
    "\tcounts=[word_uni,word_bi,word_tri]\n",
    "    model_name=\"lap\"\n",
    "\tfind_scores(te,correct_results,errors,counts,model_name)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
